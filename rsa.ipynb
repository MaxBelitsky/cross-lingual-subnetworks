{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representational similarity analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1905.06401\n",
    "\n",
    "present RSA as a vari-\n",
    "ant of pattern-information analysis, to be applied\n",
    "for understanding neural activation patterns in hu-\n",
    "man brains, for example syntactic computations\n",
    "(Tyler et al., 2013) or sensory cortical process-\n",
    "ing (Yamins and DiCarlo, 2016). The core idea\n",
    "is to find connections between data from neu-\n",
    "roimaging, behavioral experiments and computa-\n",
    "tional modeling by correlating representations of\n",
    "stimuli in each of these representation spaces via\n",
    "their pairwise (dis)similarities. RSA has also been\n",
    "used for measuring similarities between neural-\n",
    "network representation spaces ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://github.com/gchrupala/ursa/blob/master/ursa/regress.py\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "\n",
    "class Regress:\n",
    "    default_alphas = [ 10**n for n in range(-3, 2) ]\n",
    "    metrics = dict(mse       = make_scorer(mean_squared_error, greater_is_better=False),\n",
    "                   r2        = make_scorer(r2_score, greater_is_better=True),\n",
    "                   pearson_r = make_scorer(pearson_r_score, greater_is_better=True))\n",
    "\n",
    "\n",
    "    def __init__(self, cv=10, alphas=default_alphas):\n",
    "        self.cv = cv\n",
    "        self.grid =  {'alpha': alphas }\n",
    "        self._model = GridSearchCV(Ridge(), self.grid, scoring=self.metrics, cv=self.cv, return_train_score=False, refit=False)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self._model.fit(X, Y)\n",
    "        result = { name: {} for name in self.metrics.keys() }\n",
    "        for name, scorer in self.metrics.items():\n",
    "            mean = self._model.cv_results_[\"mean_test_{}\".format(name)]\n",
    "            std  = self._model.cv_results_[\"std_test_{}\".format(name)]\n",
    "            best = mean.argmax()\n",
    "            result[name]['mean'] = mean[best] * scorer._sign\n",
    "            result[name]['std']  = std[best]\n",
    "            result[name]['alpha'] = self.grid['alpha'][best]\n",
    "        self._report = result\n",
    "\n",
    "    def fit_report(self, X, Y):\n",
    "        self.fit(X, Y)\n",
    "        return self.report()\n",
    "\n",
    "    def report(self):\n",
    "        return self._report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://github.com/gchrupala/correlating-neural-and-symbolic-representations-of-language/blob/master/rsa/correlate.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def rsa(A, B):\n",
    "    \"Returns the correlation between the similarity matrices for A and B.\"\n",
    "    M_A = cosine_matrix(A, A)\n",
    "    M_B = cosine_matrix(B, B)\n",
    "    return pearson(triu(M_A), triu(M_B), dim=0)\n",
    "\n",
    "\n",
    "def cosine_matrix(U, V):\n",
    "    \"Returns the matrix of cosine similarity between each row of U and each row of V.\"\n",
    "    U_norm = U / U.norm(2, dim=1, keepdim=True)\n",
    "    V_norm = V / V.norm(2, dim=1, keepdim=True)\n",
    "    return U_norm @ V_norm.t()\n",
    "\n",
    "\n",
    "def triu(x):\n",
    "    \"Extracts upper triangular part of a matrix, excluding the diagonal.\"\n",
    "    ones = torch.ones_like(x)\n",
    "    return x[torch.triu(ones, diagonal=1) == 1]\n",
    "\n",
    "\n",
    "def pearson(x, y, dim=0, eps=1e-8):\n",
    "    \"Returns Pearson's correlation coefficient.\"\n",
    "    x1 = x - torch.mean(x, dim)\n",
    "    x2 = y - torch.mean(y, dim)\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return w12 / (w1 * w2).clamp(min=eps)\n",
    "\n",
    "\n",
    "def cor(a, b):\n",
    "    return pearson(C.triu(a), C.triu(b), dim=0).item()\n",
    "\n",
    "\n",
    "def rsa_report(data_enc1: dict, data_enc2: dict, cv=10):\n",
    "    \"\"\"Compute RSA and RSA_regress scores for two different encoders.\"\"\"\n",
    "\n",
    "    # RSA\n",
    "    D_rep = 1 - cosine_matrix(data_enc[\"test\"], data_enc[\"test\"])\n",
    "    D_rep2 = 1 - cosine_matrix(data_enc2[\"test\"], data_enc2[\"test\"])\n",
    "    rsa = cor(D_rep, D_rep2)\n",
    "\n",
    "    # RSA_regress\n",
    "    D_rep = (\n",
    "        1 - C.cosine_matrix(data_enc[\"test\"], data_enc[\"ref\"]).detach().cpu().numpy()\n",
    "    )\n",
    "    D_rep2 = (\n",
    "        1 - C.cosine_matrix(data_enc2[\"test\"], data_enc2[\"ref\"]).detach().cpu().numpy()\n",
    "    )\n",
    "    r = Regress(cv=cv)\n",
    "    rsa_reg = r.fit_report(D_rep, D_rep2)\n",
    "    return dict(rsa=rsa, rsa_regress=rsa_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RSA measures correlation\n",
    "between similarities in two different representa-\n",
    "tions globally, i.e. how close they are in their total-\n",
    "ity. In contrast, diagnostic models answer a more\n",
    "specific question: to what extent a particular type\n",
    "of information can be extracted from a given rep-\n",
    "resentation. For example, while for a particular\n",
    "neural encoding of sentences it may be possible to\n",
    "predict the length of the sentence with high accu-\n",
    "racy, the RSA between this representation and the\n",
    "strings represented only by their length may be rel-\n",
    "atively small in magnitude, since the neural repre-\n",
    "sentation may be encoding many other aspects of\n",
    "the input in addition to its length\n",
    "\n",
    "\n",
    "The scores according to RSA in some cases show a different picture. This is expected, as RSA answers a substantially different question than the other two approaches: it looks at how the whole representations match in their similarity structure, whereas both the diagnostic model and RSAREGRESS focus on the part of the representation that encodes the target information the strongest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/gchrupala/correlating-neural-and-symbolic-representations-of-language/blob/master/rsa/report.py\n",
    "\n",
    "def run_rsa():\n",
    "#   try:\n",
    "#       data_sent = json.load(open(\"data/out/ewt.json\"))\n",
    "#   except FileNotFoundError:\n",
    "#       S.ewt_json()\n",
    "#       data_sent = json.load(open(\"data/out/ewt.json\"))\n",
    "    try:\n",
    "        data = torch.load(\"data/out/ewt_embed.pt\")\n",
    "    except FileNotFoundError:\n",
    "        S.ewt_embed()\n",
    "        data = torch.load(\"data/out/ewt_embed.pt\")\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result[alpha] = dict(bow=dict(), bert=dict(), bert24=dict(), infersent=dict())\n",
    "\n",
    "    data_enc_bow = dict(test=data['bow']['test'], ref=data['bow']['ref'])\n",
    "    result[alpha]['bow']=RSA_report(data_tk, data_enc_bow)\n",
    "    result[alpha]['bert']=dict(random={}, trained={})\n",
    "    result[alpha]['bert24']=dict(random={}, trained={})\n",
    "    result[alpha]['infersent'] = dict(random={}, trained={})\n",
    "\n",
    "    for mode in ['random', 'trained']:\n",
    "        for step in ['first', 'last']:\n",
    "            result[alpha]['bert'][mode][step] = {}\n",
    "            result[alpha]['bert24'][mode][step] = {}\n",
    "            for layer in range(12):\n",
    "                logging.info(\"Computing RSA/RSA_regress scores for {} {} {}\".format(mode, step, layer))\n",
    "                data_enc = dict(test=data['bert']['test'][mode][layer][step], ref=data['bert']['ref'][mode][layer][step])\n",
    "                result[alpha]['bert'][mode][step][layer] = RSA_report(data_tk, data_enc)\n",
    "            for layer in range(24):\n",
    "                logging.info(\"Computing RSA/RSA_regress scores for {} {} {}\".format(mode, step, layer))\n",
    "                data_enc = dict(test=data['bert24']['test'][mode][layer][step], ref=data['bert24']['ref'][mode][layer][step])\n",
    "                result[alpha]['bert24'][mode][step][layer] = RSA_report(data_tk, data_enc)\n",
    "\n",
    "        result[alpha]['infersent'][mode] = RSA_report(data_tk, dict(test=data['infersent']['test'][mode], ref=data['infersent']['ref'][mode]))\n",
    "    json.dump(result, open(\"report/RSA_natural.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://github.com/gchrupala/correlating-neural-and-symbolic-representations-of-language/blob/master/rsa/synsem.py\n",
    "\n",
    "def ewt_embed():\n",
    "    \"\"\"Compute BoW, BERT and Infersent embeddings for the EWT data and save to file.\"\"\"\n",
    "    import rsa.pretrained as Pre\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    def container():\n",
    "        return dict(test=dict(random=dict(), trained=dict()),\n",
    "                    ref=dict(random=dict(), trained=dict()))\n",
    "    data = json.load(open(\"data/out/ewt.json\"))\n",
    "    emb = dict(bow={}, bert=container(), bert24=container(), infersent=container())\n",
    "    # BOW\n",
    "    v = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "    sent_ref = [s['sent'] for s in data['ref'] ]\n",
    "    sent_test = [s['sent'] for s in data['test'] ]\n",
    "    v.fit( sent_ref + sent_test )\n",
    "    emb['bow']['test'] = torch.tensor(v.transform(sent_test).toarray(), dtype=torch.float)\n",
    "    emb['bow']['ref'] =  torch.tensor(v.transform(sent_ref).toarray(), dtype=torch.float)\n",
    "\n",
    "    for split in ['test', 'ref']:\n",
    "      sent = [ datum['sent'] for datum in data[split] ]\n",
    "      for mode in [\"random\", \"trained\"]:\n",
    "        if mode == \"random\":\n",
    "            rep24 = list(Pre.encode_bert(sent, trained=False, large=True))\n",
    "            rep = list(Pre.encode_bert(sent, trained=False))\n",
    "            emb['infersent'][split][mode] = Pre.encode_infersent(sent, trained=False)\n",
    "        else:\n",
    "            rep24 = list(Pre.encode_bert(sent, trained=True, large=True))\n",
    "            rep = list(Pre.encode_bert(sent, trained=True))\n",
    "            emb['infersent'][split][mode] =  Pre.encode_infersent(sent, trained=True)\n",
    "\n",
    "        pooled24 = torch.cat([ pooled for _, pooled in rep24 ])\n",
    "        pooled = torch.cat([ pooled for _, pooled in rep ])\n",
    "        emb['bert24'][split][mode]['pooled'] = pooled24\n",
    "        emb['bert'][split][mode]['pooled'] = pooled\n",
    "        for i in range(len(rep24[0][0])):\n",
    "            emb['bert24'][split][mode][i] = {}\n",
    "            emb['bert24'][split][mode][i]['summed'] = torch.cat([ layers[i].sum(dim=1) for layers, _ in rep24 ], dim=0)\n",
    "            emb['bert24'][split][mode][i]['first']  = torch.cat([ layers[i][:,0,:] for layers, _ in rep24], dim=0)\n",
    "            emb['bert24'][split][mode][i]['last']   = torch.cat([ layers[i][:,-1,:] for layers, _ in rep24], dim=0)\n",
    "\n",
    "        for i in range(len(rep[0][0])):\n",
    "            emb['bert'][split][mode][i] = {}\n",
    "            emb['bert'][split][mode][i]['summed'] = torch.cat([ layers[i].sum(dim=1) for layers, _ in rep ], dim=0)\n",
    "            emb['bert'][split][mode][i]['first']  = torch.cat([ layers[i][:,0,:] for layers, _ in rep], dim=0)\n",
    "            emb['bert'][split][mode][i]['last']   = torch.cat([ layers[i][:,-1,:] for layers, _ in rep], dim=0)\n",
    "    torch.save(emb, \"data/out/ewt_embed.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
